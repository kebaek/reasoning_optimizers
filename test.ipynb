{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "from peft import PeftModel\n",
    "from utils import *\n",
    "from huggingface_params import cache_dir, use_auth_token\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm \n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbaek/miniconda3/envs/sphere/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/kbaek/miniconda3/envs/sphere/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2', \n",
    "                                          cache_dir=cache_dir,\n",
    "                                          use_auth_token = use_auth_token,\n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2', \n",
    "                                            cache_dir=cache_dir,\n",
    "                                            use_auth_token = use_auth_token,\n",
    "                                            trust_remote_code=True)\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "\n",
    "test_questions = dataset[\"test\"][\"question\"]\n",
    "test_answers = dataset[\"test\"]['answer']\n",
    "eval_questions = test_questions\n",
    "eval_questions = [question + \"\\nAnswer:\" for question in eval_questions]\n",
    "eval_answers = test_answers\n",
    "\n",
    "n = 5\n",
    "temperature=0.8\n",
    "max_tokens=512\n",
    "top_p=0.95\n",
    "seed=0\n",
    "stop=\"\\nDone.\"\n",
    "bad_words_ids = [tokenizer(stop).input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "samples = []\n",
    "for x in test_questions:\n",
    "  tokens = tokenizer(x, return_tensors='pt').to('cuda')\n",
    "  prediction = tokenizer.decode(model.generate(**tokens, temperature=temperature, max_new_tokens=max_tokens, top_p=top_p, bad_words_ids=bad_words_ids)[0])\n",
    "  samples.append(prediction)\n",
    "  break\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0])\n",
    "x.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "  return 5 + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = loss(x)\n",
    "L.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/locus/large_training_datsets/kbaek/ckpts/gsm8k_orig_5epochs_full_lr2e-05_rho0_bs128_lora128_meta-llama-3-8b/checkpoint-233/adapter_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m accs \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/locus/large_training_datsets/kbaek/ckpts/gsm8k_orig_5epochs_full_lr2e-05_rho0_bs128_lora128_meta-llama-3-8b/checkpoint-233/adapter_config.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sphere/lib/python3.12/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/locus/large_training_datsets/kbaek/ckpts/gsm8k_orig_5epochs_full_lr2e-05_rho0_bs128_lora128_meta-llama-3-8b/checkpoint-233/adapter_config.json'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "accs = np.load('/data/locus/large_training_datsets/kbaek/ckpts/gsm8k_orig_5epochs_full_lr2e-05_rho0_bs128_lora128_meta-llama-3-8b/checkpoint-233/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
